{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from src.train_mlm import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'mlm', 'mlruns_dir': 'mlruns', 'overwrite_data_cache': True, 'data_path': 'data', 'data_subfolder': 'data/all_data_en', 'download_url': 'https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz', 'task': 'informativeness', 'language': 'EN', 'seed': 42, 'gpu': {'fp16': True, 'fp16_opt_level': 'O2', 'half_precision_backend': 'auto'}, 'debugging_mode': False, 'mode': {'name': 'train', 'do_train': True, 'do_eval': True, 'continue_training': False, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'learning_rate': 3e-05, 'weight_decay': 0.1, 'num_train_epochs': 60, 'pad_to_max_length': False, 'max_seq_length': 'None', 'mlm_probability': 0.15, 'evaluation_strategy': 'steps', 'eval_steps': 500, 'logging_steps': 5000, 'early_stopping': True, 'patience': 50, 'save_model': True}, 'model': {'name': 'distilbert', 'pretrained_model': 'distilbert-base-uncased'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset crisis_bench_dataset (/export/home/kraft/.cache/huggingface/datasets/crisis_bench_dataset/informativeness/1.0.0/2e8a367b9209a9e88eb3aeca65dc626e3d123def2e180cfec317671f187fa562)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59719ac06b304fd6bb141a307a3724e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 109796\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16008\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 31095\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "    print(cfg)\n",
    "\n",
    "dataset = get_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla finishes first of many solar projects in Puerto Rico Read more : http http\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataset['test']['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = \"../data/data/all_data_en/crisis_consolidated_informativeness_filtered_lang_en_test.tsv\"\n",
    "delim = \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla finishes first of many solar projects in Puerto Rico Read more: https://t.co/YOeeecGl7D https://t.co/FD1dYnGhDu not_informative\n",
      "@LilyUnaSmith I'm like 2 miles away. We always get the day off for the marathon. I can see the area from my window. not_informative\n",
      "RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st explosion reported on North side of Boylston Street by the the finish line. 2nd explosion sec ... informative\n",
      "Make that 5 co-workers thanks to Bailee. not_informative\n",
      "BRING ALL YOUR WORRIES TO HIM BECAUSE HE CARES FOR YOU ���  #AyokoNaSana #ThingsToDoPagMalamig #ForevermoreTheGrandeBash #RubyPH not_informative\n",
      "\"#Intern #Accounting Internship (Finance and Economics) - San Antonio, TX Job: Scottrade - S... http://t.co/wPlYA03WBK #Texas #Internship\" not_informative\n",
      "My wish for you is that this life becomes all that you want it to not_informative\n",
      "Loan Upheaval Is The Way In Which Oneself Can Save Your House Leaving out Being Foreclosed On...TEJc not_informative\n",
      "Spring in Paris be like @ Montmartre https://t.co/I1OEqM69D0 not_informative\n",
      ".@UNOSAT: Over 15 million people exposed to strong shaking after #Mexicoearthquake. Report: https://t.co/HmlLW2iXuY https://t.co/MgHlcu6zPQ informative\n",
      "Update: Vanuatu Hit Hard by Cyclone http://t.co/uss9ByNnSe informative\n",
      "RT @911BUFF: BOSTON: FBI CONFIRMS 2ND SUSPECT ON THE LOOSE! ADDITIONAL ARMORED VEHICLES, BOMB SQUAD AND HOMELAND SECURITY HEADING TO THE SC… informative\n",
      "how is twitter fun if all you do is recycle tweets .. not_informative\n",
      "Stephen's whizz pig deluxe stacker http://t.co/jA9pjo3gND not_informative\n",
      "Softball game tonight with @danielsess #intramurals not_informative\n",
      "RT @timmytink: Colorado wildfire: Extreme weather conditions stoke High Park Fire, tax firefighters http://t.co/TIIaLAVY via @denverpost informative\n",
      "SEALED The Nuclear Regulatory Commission Reactor 1980 Record New Wave Art Rock http://t.co/FS9w33aczP http://t.co/7sywMf0EQU not_informative\n",
      "in what place is The National archive? to give the I date that he/it takes his/its function not_informative\n",
      "RT @lisabelkin: You couldn't make this stuff up: Runner witnesses Marathon explosion, returns to TX and sees fertilizer explosion. http: ... informative\n",
      "My new sounds: War Zone https://t.co/hNXRfqRk3P on #SoundCloud not_informative\n",
      "They also gave out a calendar that says that 26 July, tomorrow, is the deadline for people who wish to become Presidential candidates to file their nomination papers. not_informative\n"
     ]
    },
    {
     "data": {
      "text/plain": "['RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st explosion reported on North side of Boylston Street by the the finish line. 2nd explosion sec ...',\n '.@UNOSAT: Over 15 million people exposed to strong shaking after #Mexicoearthquake. Report: https://t.co/HmlLW2iXuY https://t.co/MgHlcu6zPQ',\n 'Update: Vanuatu Hit Hard by Cyclone http://t.co/uss9ByNnSe',\n 'RT @911BUFF: BOSTON: FBI CONFIRMS 2ND SUSPECT ON THE LOOSE! ADDITIONAL ARMORED VEHICLES, BOMB SQUAD AND HOMELAND SECURITY HEADING TO THE SC…',\n 'RT @timmytink: Colorado wildfire: Extreme weather conditions stoke High Park Fire, tax firefighters http://t.co/TIIaLAVY via @denverpost',\n \"RT @lisabelkin: You couldn't make this stuff up: Runner witnesses Marathon explosion, returns to TX and sees fertilizer explosion. http: ...\"]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.TweetNormalizer import normalizeTweet\n",
    "\n",
    "texts = []\n",
    "with open(test_set_path, \"r\", newline=None, encoding='utf-8', errors='replace') as f:\n",
    "    next(f)\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 20:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        row = line.split(delim)\n",
    "        text = row[3].strip()\n",
    "        # text = normalizeTweet(text)\n",
    "        label = row[6]\n",
    "        print(text, label)\n",
    "        if label == \"not_informative\":\n",
    "            continue\n",
    "        texts.append(text)\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForWholeWordMask\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalize=True)\n",
    "tokenized_texts = [tokenizer(txt,\n",
    "                         padding=True,\n",
    "                         truncation=True, max_length=None) for txt in texts]\n",
    "data_collator = DataCollatorForWholeWordMask(tokenizer, 0.15)\n",
    "batched = data_collator(tokenized_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "11"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_9772/2635917726.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmask_filler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fill-mask\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"vinai/bertweet-base\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmask_filler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatched\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/crisis-lm-env/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m             \u001B[0;34m-\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m**\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;34m-\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mpredicted\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mto\u001B[0m \u001B[0mreplace\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mmasked\u001B[0m \u001B[0mone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m         \"\"\"\n\u001B[0;32m--> 225\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    226\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/crisis-lm-env/lib/python3.9/site-packages/transformers/pipelines/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1101\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miterate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1102\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1103\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_single\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1105\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_multi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/crisis-lm-env/lib/python3.9/site-packages/transformers/pipelines/base.py\u001B[0m in \u001B[0;36mrun_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1108\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_single\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1109\u001B[0;31m         \u001B[0mmodel_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpreprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1110\u001B[0m         \u001B[0mmodel_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mforward_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1111\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpostprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/crisis-lm-env/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py\u001B[0m in \u001B[0;36mpreprocess\u001B[0;34m(self, inputs, return_tensors, **preprocess_parameters)\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mreturn_tensors\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m             \u001B[0mreturn_tensors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 82\u001B[0;31m         \u001B[0mmodel_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_tensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     83\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_exactly_one_mask_token\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodel_inputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/crisis-lm-env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2385\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2386\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_is_valid_text_input\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2387\u001B[0;31m             raise ValueError(\n\u001B[0m\u001B[1;32m   2388\u001B[0m                 \u001B[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2389\u001B[0m                 \u001B[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"vinai/bertweet-base\")\n",
    "mask_filler(batched['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.9371997714042664,\n  'token': 15839,\n  'token_str': 'e x p l o s i o n',\n  'sequence': 'RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st explosion reported on North side of Boylston Street by the the finish line. 2nd explosion sec...'},\n {'score': 0.01401183195412159,\n  'token': 4472,\n  'token_str': 'b l a s t',\n  'sequence': 'RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st blast reported on North side of Boylston Street by the the finish line. 2nd explosion sec...'},\n {'score': 0.01373971812427044,\n  'token': 39021,\n  'token_str': 'e x p l o s i o n s',\n  'sequence': 'RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st explosions reported on North side of Boylston Street by the the finish line. 2nd explosion sec...'},\n {'score': 0.006707529537379742,\n  'token': 39566,\n  'token_str': 'E x p l o s i o n',\n  'sequence': 'RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st Explosion reported on North side of Boylston Street by the the finish line. 2nd explosion sec...'},\n {'score': 0.004978061653673649,\n  'token': 5058,\n  'token_str': 'a l a r m',\n  'sequence': 'RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st alarm reported on North side of Boylston Street by the the finish line. 2nd explosion sec...'}]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_filler(\"RT @NBCSN: BOSTON MARATHON EXPLOSIONS: 1st <mask> reported on North side of \"\n",
    "            \"Boylston Street by the the finish line. 2nd explosion sec ...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "crisis-lm-env",
   "language": "python",
   "display_name": "crisis-lm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}