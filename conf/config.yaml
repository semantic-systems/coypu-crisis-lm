## Options needed
# Finetuning style/ architecture: via downstream task (replicate CrisisBERT), via MLM, via Adapter
# Datasets
# Loss function
# Pre-trained model from huggingface
# Language

defaults:
  - mode: train # train, test, predict
  - model: distilbert # bert, roberta


architecture: mlm # mlm, seq = sequence classification (& later adapter)
model_dir: models

overwrite_data_cache: False
data_path: data # high-level data folder
data_subfolder: data/all_data_en # experiment-specific subfolder of the crisis-bench dataset
download_url: https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz
task: informativeness # informativeness or humanitarian
language: EN # EN or DE (latter to be implemented)
seed: 42

gpu:
    fp16: True
    fp16_opt_level: O2 # letter O not number zero # info on https://nvidia.github.io/apex/amp.html
    half_precision_backend: auto