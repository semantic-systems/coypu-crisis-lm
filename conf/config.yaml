# Options needed
# Finetuning style/ architecture: via downstream task (replicate CrisisBERT), via MLM, via Adapter
# Datasets
# Loss function
# Pre-trained model from huggingface
# Language

defaults:
  - _self_
  - mode: mlm_inference # train, mlm_inference
  - model: bertweet # see 'model' config subfolder for options


architecture: mlm # mlm, seq = sequence classification (& later adapter)
mlruns_dir: mlruns

overwrite_data_cache: True
data_path: data # high-level data folder
data_subfolder: data/all_data_en # experiment-specific subfolder of the crisis-bench dataset
download_url: https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz
task: mlm # mlm, informativeness or humanitarian
language: EN # EN or DE (latter to be implemented)
model_seed: 42
data_seed: 42

gpu:
    fp16: True
    fp16_opt_level: O2 # letter O not number zero # info on https://nvidia.github.io/apex/amp.html
    half_precision_backend: auto

debugging_mode: False # in debugging mode, the whole pipeline is run on a few samples
