## Options needed
# Finetuning style/ architecture: via downstream task (replicate CrisisBERT), via MLM, via Adapter
# Datasets
# Loss function
# Pre-trained model from huggingface
# Language

defaults:
  - mode: train # train, test, predict
  - model: bert # bert, roberta


architecture: mlm # mlm, seq = sequence classification (& later adapter)
model_dir: models

overwrite_data_cache: False
data_path: data # high-level data folder
data_subfolder: data/all_data_en # experiment-specific subfolder of the crisis-bench dataset
download_url: https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz
task: informativeness # informativeness or humanitarian
language: EN # EN or DE (latter to be implemented)